source = "https://huggingface.co/datasets/nlile/math_benchmark_test_saturation"

[splits.all]
"GPT-3 175B" = 5.2
GPT-4o = 76.6
"Claude 3.5 Sonnet" = 71.1
"Gemini 2.0 Flash Experimental" = 89.7
"Qwen2.5-Math-72B-Instruct" = 88.1
"GPT-4 Turbo" = 87.92
"Qwen2.5-Math-7B-Instruct" = 85.2
Qwen2-Math-72B-Instruct = 84.0
"Qwen2.5-Math-1.5B-Instruct" = 79.9
"OpenMath2-Llama3.1-70B" = 79.6
"OpenMath2-Llama3.1-8B" = 76.1
LogicNet = 71.2
Qwen2-72B-Instruct-Step-DPO = 70.8
"AlphaMath-7B-SBS@3" = 66.3
"Minerva 62B" = 64.9
DAMOMath-7B = 64.5
MMOS-DeepSeekMath-7B = 63.7
"GPT-4-code model" = 60.8
CR = 58.0
SKiC = 56.4
DART-Math-Llama3-70B-Prop2Diff = 56.1
DART-Math-Llama3-70B-Uniform = 54.9
PHP = 53.9
DART-Math-DSMath-7B-Prop2Diff = 53.6
"Gemini Ultra" = 53.2
DART-Math-DSMath-7B-Uniform = 52.9
DeepSeekMATH-RL-7B = 51.7
AlphaLLM = 51.0
"Minerva 540B" = 50.3
MMOS-CODE-34B = 49.5
DeepSeekMath-7B-KPMath-Plus = 48.8
"PaLM 2" = 48.8
Llemma-34B-KPMath-Plus = 48.6
"Shepherd + DeepSeek-67B" = 48.1
"Minerva 8B" = 47.6
Mistral-7B-KPMath-Plus = 46.8
DART-Math-Llama3-8B-Prop2Diff = 46.6
DART-Math-Mistral-7B-Prop2Diff = 45.5
DART-Math-Llama3-8B-Uniform = 45.3
MathCoder-CL-34B = 45.2
MathCoder-L-34B = 45.1
MMIQC-72B = 45.0
MMOS-CODE-7B = 44.3
"Shepherd+Mistral-7B" = 43.5
DART-Math-Mistral-7B-Uniform = 43.5
GPT-4 = 42.5
SFT-Mistral-7B = 41.8
Llama2-13B-KPMath-Plus = 41.0
MathCoder-CL-13B = 35.9
MuggleMATH-70B = 35.6
"Shepherd + Mistral-7B" = 33.0
"WizardMath-7B-V1.1" = 33.0
"Gemini Pro" = 32.6
MuggleMATH-13B = 30.7
MathCoder-CL-7B = 30.2
MathCoder-L-13B = 29.9
Qwen2idae-16x14B = 29.9
"OpenChat-3.5-1210 7B" = 28.9
"OpenChat-3.5 7B" = 28.6
"Mixtral 8x7B" = 28.4
"MetaMath 70B" = 26.0
"MuggleMATH 7B" = 25.8
MathCoder-L-7B = 23.3
"WizardMath-70B-V1.0" = 22.7
"Camelidae-8Ã—34B" = 22.6
"MetaMath 13B" = 22.5
"davinci-002 175B" = 19.1
"Branch-Train-MiX 4x7B" = 17.8
"GAL 120B" = 16.6
"LLaMA 33B-maj1@k" = 15.2
"WizardMath-13B-V1.0" = 14.0
"LLaMA 65B" = 10.6
"GAL 30B" = 12.7
"Mistral 7B" = 13.1
"GAL 30B <work>" = 11.4
"WizardMath-7B-V1.0" = 10.7
